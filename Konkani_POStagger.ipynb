{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Konkani_POStagger.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJ1aBJ9lhLr6OqyX1iGul3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utOJW_Cb_wI_",
        "outputId": "c62885af-8df5-4659-cb5d-a1d93e49ff27"
      },
      "source": [
        "import os\n",
        "from sklearn import svm\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split \n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "%pip install sklearn_crfsuite\n",
        "from sklearn_crfsuite import CRF, scorers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn_crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.41.1)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfvLJnWa_0te",
        "outputId": "c0b8f619-69f0-4d0c-a2f7-950890295442"
      },
      "source": [
        "#mount\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYeY7Lmy_7Rv",
        "outputId": "4506494c-a576-4b4a-e134-9ffe89fdba3e"
      },
      "source": [
        "#move to current working directory\n",
        "work_dir = '/content/drive/My Drive/Colab Notebooks/NLP/project/'\n",
        "os.chdir(work_dir)\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100001.txt  100002.txt  \u001b[0m\u001b[01;34mkok\u001b[0m/  konkani.pos  Konkani_POStagger.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFP3i5S5G5A6"
      },
      "source": [
        " with open('konkani.pos','r',encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "        print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub0tFjvw_hgy"
      },
      "source": [
        "#Class to read dataset\n",
        "class Data:\n",
        "  def __init__(self,fname):  \n",
        "    self.pos_data = []\n",
        "    self.training_sentences =[] \n",
        "    self.test_sentences=[]\n",
        "\n",
        "    with open(fname,'r',encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "        self.pos_data.append(line.strip('\\n')[:-1])\n",
        "\n",
        "  #Splitting the data into train and test\n",
        "  def split_data(self,split_ratio):\n",
        "    split_index = int(len(self.pos_data) * split_ratio)\n",
        "    train_data =  self.pos_data[:split_index]\n",
        "    test_data = self.pos_data[split_index:]\n",
        "    return train_data,test_data\n",
        "\n",
        "  #Separating the token and labels for the training and test dataset\n",
        "  def create_train_data(self,data,test=False):\n",
        "    for line in data:\n",
        "      if test:\n",
        "        self.test_sentences.append([(x[0],x[1]) for word in line.split(' ') for x in [word.rsplit('/', 1)]])\n",
        "      else:\n",
        "        self.training_sentences.append([(x[0],x[1]) for word in line.split(' ') for x in [word.rsplit('/', 1)]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj5Sy5off8sU"
      },
      "source": [
        "data = Data('konkani.pos')\n",
        "train_data , test_data = data.split_data(0.9)\n",
        "data.create_train_data(train_data)\n",
        "data.create_train_data(test_data,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwpQa6UyNewC",
        "outputId": "0cfa7aa0-4f93-452a-ee46-79431cd35146"
      },
      "source": [
        "len(data.training_sentences), len(data.test_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(686, 77)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovkE-ui4_6II"
      },
      "source": [
        "#Class to define features for the CRF Model\n",
        "class Features:\n",
        "  def __init__(self):\n",
        "    self.features = dict()\n",
        "\n",
        "  def _is_punctuation(self,word):\n",
        "    if word in string.punctuation:\n",
        "      return \"True\"\n",
        "    else:\n",
        "      return \"False\"\n",
        "\n",
        "  def create_word_features(self,sentence,i):\n",
        "    feature = defaultdict()\n",
        "    sentence = sentence\n",
        "    word = sentence[i][0]\n",
        "    tag = sentence[i][1]\n",
        "    #morphology related features\n",
        "    feature= {\n",
        "        \"bias\":1.0,\n",
        "        \"word_prefix1\":word[:1],\n",
        "        \"word_prefix2\":word[:2],\n",
        "        \"word_suffix1\":word[-1:],\n",
        "        \"word_suffix2\":word[-2:],\n",
        "        \"is_punct\": self._is_punctuation(word),\n",
        "        \"is_digit\": str(word.isdigit()),\n",
        "        \"word_length\": str(len(word)),\n",
        "        \"tag\": tag\n",
        "    }\n",
        "    #word -1 token & tag\n",
        "    if i > 0:\n",
        "      feature[\"w-1\"] = sentence[i-1][0] \n",
        "      feature[\"t-1\"] = sentence[i-1][1] \n",
        "    else: \n",
        "      feature[\"w-1\"] = '_'\n",
        "      feature[\"t-1\"] = '_'\n",
        "    #word -2 token & tag\n",
        "    if i > 1:\n",
        "      feature[\"w-2\"] = sentence[i-2][0]\n",
        "      feature[\"t-2\"] = sentence[i-2][1] \n",
        "    else:\n",
        "      feature[\"w-2\"] = '_'\n",
        "      feature[\"t-2\"] = '_'\n",
        "    \n",
        "    #word +1 token & tag\n",
        "    if i+1 < len(sentence):\n",
        "      feature[\"w+1\"] = sentence[i+1][0] \n",
        "      feature[\"t+1\"] = sentence[i+1][1] \n",
        "    else:\n",
        "      feature[\"w+1\"] = '_'\n",
        "      feature[\"t+1\"] = '_' \n",
        "    #word +2 token & tag\n",
        "    if i+2 < len(sentence):\n",
        "      feature[\"w+2\"] = sentence[i+2][0] \n",
        "      feature[\"t+2\"] = sentence[i+2][1]\n",
        "    else:\n",
        "      feature[\"w+2\"] = '_'\n",
        "      feature[\"t+2\"] = '_'\n",
        "\n",
        "    return feature\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO8FQ3UI2bbm"
      },
      "source": [
        "#Creating the feature set from training data\n",
        "crf_feature = Features()\n",
        "feature_set = []\n",
        "feature_labels = []\n",
        "for sent in data.training_sentences:\n",
        "  feature_set.append([crf_feature.create_word_features(sent,i) for i in range(len(sent))])\n",
        "  feature_labels.append([word[1] for word in sent])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_rUqgz66jT5"
      },
      "source": [
        "#Training the CRF model\n",
        "x_train_crf, x_val_crf, y_train_crf, y_val_crf = train_test_split(feature_set,feature_labels,test_size=0.1)\n",
        "crf_model = CRF(\n",
        "    algorithm ='lbfgs',\n",
        "    c1 = 0.1,\n",
        "    c2 = 0.1,\n",
        "    max_iterations = 10,\n",
        "    all_possible_transitions = True \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4hexNVvGw-B",
        "outputId": "4204178c-8af0-47d9-e1fc-0b076bfbc728"
      },
      "source": [
        "print(x_train_crf[0])\n",
        "print(y_train_crf[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'bias': 1.0, 'word_prefix1': 'द', 'word_prefix2': 'द', 'word_suffix1': 'द', 'word_suffix2': 'द', 'is_punct': 'False', 'is_digit': 'False', 'word_length': '1', 'tag': 'N-NNP', 'w-1': '_', 't-1': '_', 'w-2': '_', 't-2': '_', 'w+1': 'हांणी', 't+1': 'PR-PRP', 'w+2': '1899', 't+2': 'QT-QTC'}, {'bias': 1.0, 'word_prefix1': 'ह', 'word_prefix2': 'हा', 'word_suffix1': 'ी', 'word_suffix2': 'णी', 'is_punct': 'False', 'is_digit': 'False', 'word_length': '5', 'tag': 'PR-PRP', 'w-1': 'द', 't-1': 'N-NNP', 'w-2': '_', 't-2': '_', 'w+1': '1899', 't+1': 'QT-QTC', 'w+2': 'वर्सा', 't+2': 'N-NN'}, {'bias': 1.0, 'word_prefix1': '1', 'word_prefix2': '18', 'word_suffix1': '9', 'word_suffix2': '99', 'is_punct': 'False', 'is_digit': 'True', 'word_length': '4', 'tag': 'QT-QTC', 'w-1': 'हांणी', 't-1': 'PR-PRP', 'w-2': 'द', 't-2': 'N-NNP', 'w+1': 'वर्सा', 't+1': 'N-NN', 'w+2': 'आनी', 't+2': 'CC-CCD'}, {'bias': 1.0, 'word_prefix1': 'व', 'word_prefix2': 'वर', 'word_suffix1': 'ा', 'word_suffix2': 'सा', 'is_punct': 'False', 'is_digit': 'False', 'word_length': '5', 'tag': 'N-NN', 'w-1': '1899', 't-1': 'QT-QTC', 'w-2': 'हांणी', 't-2': 'PR-PRP', 'w+1': 'आनी', 't+1': 'CC-CCD', 'w+2': 'हें', 't+2': 'DM-DMD'}, {'bias': 1.0, 'word_prefix1': 'आ', 'word_prefix2': 'आन', 'word_suffix1': 'ी', 'word_suffix2': 'नी', 'is_punct': 'False', 'is_digit': 'False', 'word_length': '3', 'tag': 'CC-CCD', 'w-1': 'वर्सा', 't-1': 'N-NN', 'w-2': '1899', 't-2': 'QT-QTC', 'w+1': 'हें', 't+1': 'DM-DMD', 'w+2': '.', 't+2': 'RD-PUNC'}, {'bias': 1.0, 'word_prefix1': 'ह', 'word_prefix2': 'हे', 'word_suffix1': 'ं', 'word_suffix2': 'ें', 'is_punct': 'False', 'is_digit': 'False', 'word_length': '3', 'tag': 'DM-DMD', 'w-1': 'आनी', 't-1': 'CC-CCD', 'w-2': 'वर्सा', 't-2': 'N-NN', 'w+1': '.', 't+1': 'RD-PUNC', 'w+2': '_', 't+2': '_'}, {'bias': 1.0, 'word_prefix1': '.', 'word_prefix2': '.', 'word_suffix1': '.', 'word_suffix2': '.', 'is_punct': 'True', 'is_digit': 'False', 'word_length': '1', 'tag': 'RD-PUNC', 'w-1': 'हें', 't-1': 'DM-DMD', 'w-2': 'आनी', 't-2': 'CC-CCD', 'w+1': '_', 't+1': '_', 'w+2': '_', 't+2': '_'}]\n",
            "['N-NNP', 'PR-PRP', 'QT-QTC', 'N-NN', 'CC-CCD', 'DM-DMD', 'RD-PUNC']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4OB5kqfHNOO",
        "outputId": "3d588c71-bca6-4eab-d84c-ce318baaf0d8"
      },
      "source": [
        "crf_model.fit(x_train_crf,y_train_crf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=True,\n",
              "    averaging=None, c=None, c1=0.1, c2=0.1, calibration_candidates=None,\n",
              "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
              "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
              "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=10,\n",
              "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
              "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTyRGwrg6TL9",
        "outputId": "f71d5459-9c42-4893-e9e9-8f43d2ccea39"
      },
      "source": [
        "#Function to calculate accuracy of predicted labels\n",
        "def calc_accuracy(y_true,y_pred):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for sent_true, sent_pred in zip(y_true, y_pred):\n",
        "      for true_label, pred_label in zip(sent_true, sent_pred):\n",
        "          if true_label == pred_label:\n",
        "              correct += 1\n",
        "          total += 1\n",
        "  return correct/total\n",
        "\n",
        "y_pred_crf = crf_model.predict(x_val_crf)\n",
        "accuracy_CRF = calc_accuracy(y_val_crf,y_pred_crf)\n",
        "print(\"CRF Model Validation Accuracy: \", accuracy_CRF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CRF Model Validation Accuracy:  0.9940357852882704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb8r4ypT3xRK",
        "outputId": "0b34016b-1ff7-4cc5-a81f-c3387f758b17"
      },
      "source": [
        "#Creating the feature set for test set\n",
        "X_test_crf=[]\n",
        "y_test_crf=[]\n",
        "for sent in data.test_sentences:\n",
        "  X_test_crf.append([crf_feature.create_word_features(sent,i) for i in range(len(sent))])\n",
        "  y_test_crf.append([word[1] for word in sent])\n",
        "\n",
        "#Testing the model for the test set\n",
        "test_pred_crf = crf_model.predict(X_test_crf)\n",
        "test_CRF = calc_accuracy(y_test_crf,test_pred_crf)\n",
        "print(\"CRF Model Test Accuracy: \", test_CRF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CRF Model Test Accuracy:  0.9832214765100671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32KwDC_SBjNN"
      },
      "source": [
        "sentence = 'पळेवन-पळेवन तुंकां कांय दिसचें ना'.split(' ')\n",
        "test = []\n",
        "test.append([crf_feature.create_word_features(sentence,i) for i in range(len(sentence))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qNh6A0oGp2g",
        "outputId": "e26aa269-1624-4a02-f2c3-e57c6d0603cc"
      },
      "source": [
        "crf_model.predict(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['CC-CCS', 'PR-PRL', 'N-NNP', 'N-NNP', 'RD-PUNC']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFYOpfbHoMKZ",
        "outputId": "b0314549-8d31-437f-974b-e059569602fd"
      },
      "source": [
        "from gensim.utils import tokenize\n",
        "from gensim.models import word2vec\n",
        "\n",
        "#list of files for raw text\n",
        "raw_txt_files = glob.glob('./kok/*.txt')\n",
        "print(len(raw_txt_files))\n",
        "EMB_DIM = 100\n",
        "# Create an UNK token for unknown words\n",
        "UNK_INDEX = 0 \n",
        "UNK_TOKEN = \"UNK\"\n",
        "\n",
        "#Class to process data for the RNNs and create word vectors\n",
        "class NeuralData:\n",
        "  def __init__(self,files):\n",
        "    self.data = []\n",
        "    self.raw_data_text = []\n",
        "    self.read_text(files)\n",
        "\n",
        "  def preprocess_text(self,text,fname):\n",
        "    #Remove the english alphabets from the raw text\n",
        "    text = re.findall(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+|[.,!?;()]\", text)\n",
        "    #remove half space characters left in the text\n",
        "    text = list(filter(('\\u200c').__ne__, text))\n",
        "    text = list(filter(('\\u200e').__ne__, text))\n",
        "    return text\n",
        "\n",
        "  def read_text(self, files):\n",
        "    count = 0\n",
        "    for file in files:\n",
        "      with open(file,'r') as f: \n",
        "        #data = f.read().splitlines()\n",
        "        self.data = f.read()\n",
        "        self.data = self.preprocess_text(self.data,f.name)\n",
        "        \n",
        "        self.raw_data_text.append(self.data)\n",
        "        count+=1\n",
        "      len(self.raw_data_text)\n",
        "\n",
        "  def create_word_vectors(self):\n",
        "    tokens = word2vec.Word2Vec(self.raw_data_text,size=EMB_DIM,window =5, min_count=1)\n",
        "    word_vectors = tokens.wv  # get trained embeddings - an KeyedVector instaces\n",
        "    return word_vectors \n",
        "\n",
        "  def data2index(self,word_set,tag_set):\n",
        "    #use indices from the word & tag list to create an index dictionary\n",
        "    word_idx = {w: i for i, w in enumerate(word_set)} \n",
        "    tag_idx = {t: i for i, t in enumerate(tag_set)}\n",
        "\n",
        "    # updating the indexes of words for the UNK token\n",
        "    word_idx = {word: (index + 1) if index >= UNK_INDEX else index \n",
        "                for word, index in word_idx.items()}\n",
        "    word_idx[UNK_TOKEN] = UNK_INDEX\n",
        "    return word_idx,tag_idx\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTEX3qItSWrr"
      },
      "source": [
        "neural_data = NeuralData(raw_txt_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2iM5J4iZSn7"
      },
      "source": [
        "#Training the RNN model \n",
        "#calculate unique list of words\n",
        "word_set = list(set([word[0] for sent in data.training_sentences for word in sent]))\n",
        "word_set.append('ENDPAD')\n",
        "\n",
        "#calculate unique list of tags\n",
        "tag_set = list(set([word[1] for sent in data.training_sentences for word in sent]))\n",
        "tag_set.append('ENDPAD')\n",
        "\n",
        "#create word and label indices \n",
        "word_idx, tag_idx = neural_data.data2index(word_set,tag_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdAzLwBZZoVe"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#the max length of the sentence set\n",
        "maxLength =max([len(sent) for sent in data.training_sentences])\n",
        "\n",
        "#the following lines will create an index vector for text and labels \n",
        "#and add padding to the sentences\n",
        "#padded sentences are assigned 'ENDPAD' tag\n",
        "X = [[word_idx[w[0]] for w in s] for s in  data.training_sentences]\n",
        "X = pad_sequences(maxlen=maxLength, sequences=X, padding=\"post\", value=len(word_set)-1)\n",
        "y = [[tag_idx[w[1]] for w in s] for s in  data.training_sentences]\n",
        "y = pad_sequences(maxlen=maxLength, sequences=y, padding=\"post\", value=tag_idx[\"ENDPAD\"])\n",
        "#create one hot vector of labels\n",
        "y = [to_categorical(i, num_classes=len(tag_set)) for i in y]\n",
        "\n",
        "#split data set into train and validation\n",
        "X_train,X_val, y_train, y_val = train_test_split(X,y,test_size=0.1,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUHZxzANY979",
        "outputId": "33193ffd-0bfb-4d79-f572-003dc1c64501"
      },
      "source": [
        "#using Keras to build an RNN\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "\n",
        "input = Input(shape=(maxLength,))\n",
        "model = Embedding(input_dim=len(word_set), output_dim=50, input_length=maxLength)(input)  # 50-dim embedding\n",
        "model = Dropout(0.1)(model)\n",
        "model = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.2))(model)  # variational biLSTM\n",
        "out = TimeDistributed(Dense(len(tag_set), activation=\"softmax\"))(model)  # softmax output layer\n",
        "\n",
        "\n",
        "#training the model\n",
        "model = Model(input, out)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "history = model.fit(X_train, np.array(y_train), batch_size=5, epochs=15, validation_data=(X_val,np.array(y_val)), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 130)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_9 (Embedding)      (None, 130, 50)           115950    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 130, 50)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_9 (Bidirection (None, 130, 128)          58880     \n",
            "_________________________________________________________________\n",
            "time_distributed_14 (TimeDis (None, 130, 37)           4773      \n",
            "=================================================================\n",
            "Total params: 179,603\n",
            "Trainable params: 179,603\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "124/124 [==============================] - 23s 189ms/step - loss: 0.4933 - accuracy: 0.8916 - val_loss: 0.2786 - val_accuracy: 0.9227\n",
            "Epoch 2/15\n",
            "124/124 [==============================] - 23s 186ms/step - loss: 0.2649 - accuracy: 0.9265 - val_loss: 0.2322 - val_accuracy: 0.9385\n",
            "Epoch 3/15\n",
            "124/124 [==============================] - 23s 186ms/step - loss: 0.2059 - accuracy: 0.9430 - val_loss: 0.1871 - val_accuracy: 0.9468\n",
            "Epoch 4/15\n",
            "124/124 [==============================] - 23s 186ms/step - loss: 0.1611 - accuracy: 0.9562 - val_loss: 0.1493 - val_accuracy: 0.9594\n",
            "Epoch 5/15\n",
            "124/124 [==============================] - 23s 186ms/step - loss: 0.1223 - accuracy: 0.9663 - val_loss: 0.1206 - val_accuracy: 0.9660\n",
            "Epoch 6/15\n",
            "124/124 [==============================] - 23s 185ms/step - loss: 0.0930 - accuracy: 0.9763 - val_loss: 0.0999 - val_accuracy: 0.9756\n",
            "Epoch 7/15\n",
            "124/124 [==============================] - 23s 186ms/step - loss: 0.0706 - accuracy: 0.9839 - val_loss: 0.0848 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "124/124 [==============================] - 23s 186ms/step - loss: 0.0528 - accuracy: 0.9883 - val_loss: 0.0746 - val_accuracy: 0.9815\n",
            "Epoch 9/15\n",
            "124/124 [==============================] - 23s 185ms/step - loss: 0.0405 - accuracy: 0.9913 - val_loss: 0.0681 - val_accuracy: 0.9817\n",
            "Epoch 10/15\n",
            "124/124 [==============================] - 23s 187ms/step - loss: 0.0310 - accuracy: 0.9935 - val_loss: 0.0631 - val_accuracy: 0.9834\n",
            "Epoch 11/15\n",
            "124/124 [==============================] - 23s 188ms/step - loss: 0.0243 - accuracy: 0.9950 - val_loss: 0.0604 - val_accuracy: 0.9844\n",
            "Epoch 12/15\n",
            "124/124 [==============================] - 23s 188ms/step - loss: 0.0194 - accuracy: 0.9959 - val_loss: 0.0584 - val_accuracy: 0.9852\n",
            "Epoch 13/15\n",
            "124/124 [==============================] - 23s 188ms/step - loss: 0.0159 - accuracy: 0.9965 - val_loss: 0.0560 - val_accuracy: 0.9854\n",
            "Epoch 14/15\n",
            "124/124 [==============================] - 23s 186ms/step - loss: 0.0124 - accuracy: 0.9974 - val_loss: 0.0583 - val_accuracy: 0.9854\n",
            "Epoch 15/15\n",
            "124/124 [==============================] - 23s 186ms/step - loss: 0.0104 - accuracy: 0.9979 - val_loss: 0.0590 - val_accuracy: 0.9842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roM48CvtOoRP"
      },
      "source": [
        "#create word index vectors for the test set.\n",
        "X_test = [[word_idx.get(w[0],word_idx['UNK']) for w in s] for s in  data.test_sentences]\n",
        "X_test_pad = pad_sequences(maxlen=maxLength, sequences=X_test, padding=\"post\", value=len(word_set) - 1)\n",
        "test_labels = [[tag_idx[w[1]] for w in s] for s in  data.test_sentences]\n",
        "test_labels_pad = pad_sequences(maxlen=maxLength, sequences=test_labels, padding=\"post\", value=tag_idx[\"ENDPAD\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEWo4B8e-vCx",
        "outputId": "e0ebd2a6-5043-4f7d-fe64-d11066bd3778"
      },
      "source": [
        "y_pred_test = model.predict(X_test_pad)\n",
        "p = np.argmax(y_pred_test,axis=-1)\n",
        "\n",
        "model_test_acc = calc_accuracy(test_labels_pad,p)\n",
        "print(\"RNN Model Test accuracy is \", model_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN Model Test accuracy is  0.9771228771228772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyXHGvWdXhfr"
      },
      "source": [
        "#RNN Model with word embeddings\n",
        "word_vectors = neural_data.create_word_vectors()\n",
        "#calculate unique list of words from the word vectors\n",
        "word_set_wv = list(set([w for w in word_vectors.vocab.keys()]))\n",
        "\n",
        "word_idx_wv = {k: v.index for k, v in word_vectors.vocab.items()}\n",
        "\n",
        "# we add one single vector for the Unknown words\n",
        "embedding_matrix = word_vectors.vectors\n",
        "unk_vector = embedding_matrix.mean(0)\n",
        "\n",
        "embedding_matrix = np.insert(embedding_matrix, [UNK_INDEX], [unk_vector], axis=0)\n",
        "\n",
        "# updating the indexes of words that follow the new word\n",
        "word_idx_wv = {word: (index + 1) if index >= UNK_INDEX else index \n",
        "            for word, index in word_idx_wv.items()}\n",
        "word_idx_wv[UNK_TOKEN] = UNK_INDEX\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYnUsR_uAris"
      },
      "source": [
        "#Red-doing the word index vectors for training sentences with the word vector vocabulary. \n",
        "#This isnt repeated for the test since the labels are the same\n",
        "X_wts = [[word_idx_wv.get(w[0],word_idx_wv['UNK']) for w in s] for s in  data.training_sentences]\n",
        "X_pad_wts = pad_sequences(maxlen=maxLength, sequences=X_wts, padding=\"post\", value=len(word_idx_wv)-1)\n",
        "\n",
        "#split data set into train and validation\n",
        "X_train_wts,X_val_wts, y_train_wts, y_val_wts = train_test_split(X_pad_wts,y,test_size=0.1,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6TjI--4bMSv",
        "outputId": "4767d9c5-ba37-4dd7-c30c-a5ce7e4af256"
      },
      "source": [
        "#RNN model using pre-trained word embeddings\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Layer\n",
        "\n",
        "vocab_length = len(embedding_matrix)\n",
        "weights_model = Sequential()\n",
        "#model.add(Input(shape=(maxLength,)))\n",
        "weights_model.add(Embedding(input_dim=vocab_length, output_dim=EMB_DIM, weights=[embedding_matrix] ,input_length=maxLength , trainable=False))\n",
        "weights_model.add(Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.2)))\n",
        "weights_model.add(TimeDistributed(Dense(64, activation=\"tanh\")))\n",
        "weights_model.add(Dropout(0.2))\n",
        "weights_model.add(TimeDistributed(Dense(len(tag_set), activation=\"softmax\")))\n",
        "weights_model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "print(weights_model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 130, 100)          13852100  \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 130, 128)          84480     \n",
            "_________________________________________________________________\n",
            "time_distributed_12 (TimeDis (None, 130, 64)           8256      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 130, 64)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_13 (TimeDis (None, 130, 37)           2405      \n",
            "=================================================================\n",
            "Total params: 13,947,241\n",
            "Trainable params: 95,141\n",
            "Non-trainable params: 13,852,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZOWDHX89Dma",
        "outputId": "5b70edb5-5943-47d3-f8d6-a23cb96fea3a"
      },
      "source": [
        "weights_model.fit(X_train_wts, np.array(y_train_wts), batch_size=10, epochs=20, validation_data=(X_val_wts, np.array(y_val_wts)), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "62/62 [==============================] - 13s 203ms/step - loss: 0.5783 - accuracy: 0.8968 - val_loss: 0.2669 - val_accuracy: 0.9268\n",
            "Epoch 2/20\n",
            "62/62 [==============================] - 12s 196ms/step - loss: 0.2658 - accuracy: 0.9245 - val_loss: 0.2343 - val_accuracy: 0.9348\n",
            "Epoch 3/20\n",
            "62/62 [==============================] - 12s 197ms/step - loss: 0.2406 - accuracy: 0.9313 - val_loss: 0.2177 - val_accuracy: 0.9370\n",
            "Epoch 4/20\n",
            "62/62 [==============================] - 12s 198ms/step - loss: 0.2280 - accuracy: 0.9349 - val_loss: 0.2116 - val_accuracy: 0.9391\n",
            "Epoch 5/20\n",
            "62/62 [==============================] - 12s 198ms/step - loss: 0.2188 - accuracy: 0.9374 - val_loss: 0.2232 - val_accuracy: 0.9353\n",
            "Epoch 6/20\n",
            "62/62 [==============================] - 12s 198ms/step - loss: 0.2128 - accuracy: 0.9388 - val_loss: 0.1998 - val_accuracy: 0.9415\n",
            "Epoch 7/20\n",
            "62/62 [==============================] - 12s 198ms/step - loss: 0.2081 - accuracy: 0.9396 - val_loss: 0.1963 - val_accuracy: 0.9437\n",
            "Epoch 8/20\n",
            "62/62 [==============================] - 12s 196ms/step - loss: 0.2029 - accuracy: 0.9411 - val_loss: 0.2030 - val_accuracy: 0.9378\n",
            "Epoch 9/20\n",
            "62/62 [==============================] - 12s 197ms/step - loss: 0.2000 - accuracy: 0.9419 - val_loss: 0.1906 - val_accuracy: 0.9445\n",
            "Epoch 10/20\n",
            "62/62 [==============================] - 12s 197ms/step - loss: 0.1962 - accuracy: 0.9430 - val_loss: 0.1908 - val_accuracy: 0.9444\n",
            "Epoch 11/20\n",
            "62/62 [==============================] - 12s 197ms/step - loss: 0.1937 - accuracy: 0.9439 - val_loss: 0.1878 - val_accuracy: 0.9449\n",
            "Epoch 12/20\n",
            "62/62 [==============================] - 12s 196ms/step - loss: 0.1902 - accuracy: 0.9441 - val_loss: 0.1865 - val_accuracy: 0.9450\n",
            "Epoch 13/20\n",
            "62/62 [==============================] - 12s 197ms/step - loss: 0.1890 - accuracy: 0.9451 - val_loss: 0.1839 - val_accuracy: 0.9450\n",
            "Epoch 14/20\n",
            "62/62 [==============================] - 12s 198ms/step - loss: 0.1868 - accuracy: 0.9455 - val_loss: 0.1810 - val_accuracy: 0.9468\n",
            "Epoch 15/20\n",
            "62/62 [==============================] - 12s 201ms/step - loss: 0.1843 - accuracy: 0.9457 - val_loss: 0.1823 - val_accuracy: 0.9450\n",
            "Epoch 16/20\n",
            "62/62 [==============================] - 12s 198ms/step - loss: 0.1827 - accuracy: 0.9461 - val_loss: 0.1797 - val_accuracy: 0.9467\n",
            "Epoch 17/20\n",
            "62/62 [==============================] - 12s 198ms/step - loss: 0.1812 - accuracy: 0.9463 - val_loss: 0.1809 - val_accuracy: 0.9441\n",
            "Epoch 18/20\n",
            "62/62 [==============================] - 12s 196ms/step - loss: 0.1798 - accuracy: 0.9466 - val_loss: 0.1792 - val_accuracy: 0.9453\n",
            "Epoch 19/20\n",
            "62/62 [==============================] - 12s 197ms/step - loss: 0.1778 - accuracy: 0.9467 - val_loss: 0.1760 - val_accuracy: 0.9466\n",
            "Epoch 20/20\n",
            "62/62 [==============================] - 12s 197ms/step - loss: 0.1762 - accuracy: 0.9476 - val_loss: 0.1722 - val_accuracy: 0.9473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f612b0cb710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPdGA9YNMBJy"
      },
      "source": [
        "#Creating the test set with the word vectors\n",
        "X_test_wts = [[word_idx_wv.get(w[0],word_idx_wv['UNK']) for w in s] for s in  data.test_sentences]\n",
        "X_testwts_pad = pad_sequences(maxlen=maxLength, sequences=X_test_wts, padding=\"post\", value=len(word_idx_wv) - 1)\n",
        "test_labels = [[tag_idx[w[1]] for w in s] for s in  data.test_sentences]\n",
        "test_labels_pad = pad_sequences(maxlen=maxLength, sequences=test_labels, padding=\"post\", value=tag_idx[\"ENDPAD\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2smeyauHBtF",
        "outputId": "c9c941c3-f79a-4f14-c7c8-60318310b9a8"
      },
      "source": [
        "y_pred_wts = weights_model.predict(X_testwts_pad)\n",
        "p_wts = np.argmax(y_pred_wts,axis=-1)\n",
        "#Calculating model accuracy on test\n",
        "model_wts_acc = calc_accuracy(test_labels_pad,p_wts)\n",
        "print(\"CNN Model with word embeddings Test accuracy is \", model_wts_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN Model with word embeddings Test accuracy is  0.948951048951049\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}