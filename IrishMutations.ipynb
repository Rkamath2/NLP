{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "IrishMutations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2qzjcKKYCSg"
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "import numpy as np\n",
        "import csv\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL8ySITdYM5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74bc491a-8efd-4055-ba56-1ec0ea1cf729"
      },
      "source": [
        "####### (2) Mount the google drive so that we can access the files from google drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY3FDkfajhKj"
      },
      "source": [
        "work_dir = '/content/drive/My Drive/Colab Notebooks/NLP/Celtic Mutation'\n",
        "os.chdir(work_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OB9iM1EYCSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbe437e-9a44-444c-86de-6ae560baa321"
      },
      "source": [
        "#Read the train file\n",
        "#tsv_file = open(\"./train.tsv\",'r')\n",
        "df_data = pd.read_csv('train.tsv',sep='\\t', header=None,quoting = csv.QUOTE_NONE)\n",
        "df_data.columns =['text','label']\n",
        "#df_data = df_data[pd.notnull(df_data['text'])]\n",
        "df_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WEpbuDtIYCSy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36dfb20a-2985-4dac-d1d2-d6c8685947e6"
      },
      "source": [
        "#Given dataset is in the form of tokens. \n",
        "#This function will create a new dataset of (word,tag) tuples for each sentence \n",
        "def create_sentence_set(data):\n",
        "    #extracting sentence start indices\n",
        "    sent_start = list(np.where(df_data['text'] == '<S>')[0])\n",
        "    sent_start.insert(0,0)  #First sentence doesnt have a start tag so add it manually\n",
        "    sent_start.append(len(df_data))\n",
        "    dataset_sent = []\n",
        "    tag_set = []\n",
        "    word_set = []\n",
        "    for i in range(len(sent_start)-1): \n",
        "        sos =  sent_start[i]    #start of sentence index\n",
        "        eos =  sent_start[i+1]  #end of sentence index\n",
        "        #create a sentence dataset with word,tag tuples\n",
        "        dataset_sent.append([(data.iloc[i,0],data.iloc[i,1]) for i in range(sos,eos)] ) \n",
        "        tag_set.append([data.iloc[i,1] for i in range(sos,eos)] ) \n",
        "    return dataset_sent , tag_set \n",
        "        \n",
        "dataset,tag_set = create_sentence_set(df_data)\n",
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "395923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6n6elVfYCS0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a7137b2-d103-4499-96b8-533803f780d3"
      },
      "source": [
        "#splitting sentence dataset into training and validation sets in 80:20 ratio\n",
        "#def train_test_split(split_index):  \n",
        "split_index = int(len(dataset) * 0.8)\n",
        "X_train = dataset[:split_index]\n",
        "X_val = dataset[split_index:]\n",
        "print(len(X_train), len(X_val))\n",
        "train_tags = tag_set[:split_index]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "316738 79185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLDXzRQuYCTC"
      },
      "source": [
        "#Compucing frequencies for unigram, bigram and trigram\n",
        "uni_freq = {}\n",
        "bi_freq= {}\n",
        "tri_freq = {}\n",
        "def create_tagger(tagset):\n",
        "    for sent in tagset:\n",
        "        for t in range(len(sent)):\n",
        "            if (t-2 >= 0):    \n",
        "                if (sent[t-2],sent[t-1], sent[t]) not in tri_freq:\n",
        "                    tri_freq[(sent[t-2],sent[t-1], sent[t])]=1\n",
        "                else:\n",
        "                    tri_freq[(sent[t-2],sent[t-1], sent[t])]+=1\n",
        "            if (t-1 >= 0):\n",
        "                if (sent[t-1],sent[t]) not in bi_freq:\n",
        "                    bi_freq[(sent[t-1],sent[t])]=1\n",
        "                else:\n",
        "                    bi_freq[(sent[t-1],sent[t])]+=1\n",
        "            \n",
        "            if (sent[t]) not in uni_freq:\n",
        "                    uni_freq[sent[t]]=1\n",
        "            else:\n",
        "                uni_freq[sent[t]]+=1\n",
        "                \n",
        "#Adding a small value to unseen trigram sets in training              \n",
        "def trigram_smoothing():\n",
        "  tag_set = uni_freq.keys()\n",
        "  for t2 in tag_set:\n",
        "    for t1 in tag_set:\n",
        "      for t in tag_set:\n",
        "        if (t2,t1,t) not in tri_freq:\n",
        "          tri_freq[(t2,t1,t)] =0.1\n",
        "\n",
        "\n",
        "create_tagger(train_tags)\n",
        "trigram_smoothing()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oCWXEsBYCTH"
      },
      "source": [
        "#calculate probablities for unigram , bigram and trigram sequences.\n",
        "trigram_prob = {}\n",
        "bigram_prob={}\n",
        "unigram_prob={}\n",
        "\n",
        "trigram_prob = {(t2 , t1, t): tri_freq[(t2 , t1, t)]/bi_freq[t2,t1] for (t2,t1,t) in tri_freq}\n",
        "\n",
        "bigram_prob = {(t1, t): bi_freq[(t1, t)]/uni_freq[t1]  for (t1,t) in bi_freq}\n",
        "\n",
        "unigram_prob = {t: uni_freq[t]/sum(uni_freq.values()) for t in uni_freq}\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoue_I-9sLRv"
      },
      "source": [
        "#Inverse of Naive Bayes\n",
        "#Calculating probability of word given the tag\n",
        "flipped = [(t,w) for sent in X_train for (w,t) in sent]\n",
        "tag_word_count= {}\n",
        "tag_c = {'N':0,'H':0,'S':0,'T':0,'U':0}\n",
        "for (t,w) in flipped:\n",
        "  if (t,w) not in tag_word_count:\n",
        "    tag_word_count[(t,w)] = 1\n",
        "  else:\n",
        "    tag_word_count[(t,w)] += 1\n",
        "    \n",
        "  tag_c[t] += 1\n",
        "\n",
        "wordgiventag = {}\n",
        "\n",
        "wordgiventag = {(w,t) : tag_word_count[(t,w)]/uni_freq[t] for (t,w) in tag_word_count}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iKyo2DoZxe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3a61663-4a20-4349-ee84-bf1f050aff2c"
      },
      "source": [
        "wordgiventag[('a','N')]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.046853667066805396"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xugRo9JYCTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1322ea40-9601-44d1-ecad-5329cb5a3f19"
      },
      "source": [
        "#calculating the pi values \n",
        "#the initial tag probablities for each tag\n",
        "init_prob = {}\n",
        "init_freq = {}\n",
        "\n",
        "for sent_tag in tag_set:\n",
        "  if sent_tag[0] == '<S>':\n",
        "    start_tag = sent_tag[1]\n",
        "  else:\n",
        "    start_tag = sent_tag[0]\n",
        "  if start_tag not in init_freq:\n",
        "      init_freq[start_tag]=1\n",
        "  else:\n",
        "      init_freq[start_tag]+=1\n",
        "        \n",
        "init_prob = {t: count/sum(init_freq.values())  for tag,count in init_freq.items()}\n",
        "init_prob #in our dataset the initial tag is always N"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'N': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2gSYAw4YCXc"
      },
      "source": [
        "#Taken from Dr.Scannell's code \n",
        "#Viterbi algorithm tweaked to fit to trigram HMM \n",
        "def argmax(V,tag_list,t,t1,i,emission_prob):\n",
        "    ans=-1\n",
        "    best=None\n",
        "    for t2 in tag_list:\n",
        "      #calculations for second word where t-2 doesnt exist\n",
        "      if i ==1:\n",
        "        temp=V.get(('',t1,i-1),0) * bigram_prob.get((t1,t)) * emission_prob\n",
        "        if temp > ans:\n",
        "            ans = temp\n",
        "            best = t2\n",
        "      else:   #calculating for words index > 2\n",
        "        temp=V.get((t2,t1,i-1)) * trigram_prob.get((t2,t1,t),0) * emission_prob\n",
        "        if temp > ans:\n",
        "            ans = temp\n",
        "            best = t2\n",
        "    return (best,ans)\n",
        "\n",
        "def printV(sentence,tag_list,V,B):\n",
        "    #print('i='+str(0)+' ['+sentence[0]+']')\n",
        "    #print('  N'+'='+str(V[('','N',0)]))\n",
        "    for i in range(1,len(sentence)):\n",
        "        #print('i='+str(i)+' ['+sentence[i]+']')\n",
        "        for t1 in tag_list:\n",
        "          for t in tag_list:\n",
        "              if V[(t1,t,i)] != 0:\n",
        "                  toprint='  '+t+'='+str(V[(t1,t,i)])\n",
        "                  if i>0:\n",
        "                      toprint += ' (from '+B[(t,i)]+')'\n",
        "                  return toprint\n",
        "                \n",
        "def viterbi(sentence):\n",
        "    cur_vit = []\n",
        "    prev_viterbi= []\n",
        "    V = dict()    # keys are (t,i) where t is a tag (row label) and i is position in sentence (column label)\n",
        "    Back_ptr = dict()    # same keys as V; this stores the \"backpointers\" to remember best tag sequence\n",
        "    tag_list = uni_freq.keys()\n",
        "    backpointer = []\n",
        "    \n",
        "    for t in 'N': #first tag is always N\n",
        "      V[('',t,0)] = 1.0 * wordgiventag.get((sentence[0],t),-1)\n",
        "      cur_vit.append(V)\n",
        "      backpointer.append('N')\n",
        "      #print(V)\n",
        "    for i in range(1,len(sentence)):\n",
        "     this_viterbi = { }\n",
        "     prev_viterbi = cur_vit[-1]\n",
        "     for t1 in tag_list:\n",
        "        for t in tag_list:\n",
        "         emission_prob = wordgiventag.get((sentence[i],t),-1)\n",
        "         pair = argmax(V,tag_list,t,t1,i,emission_prob)\n",
        "         Back_ptr[(t,i)] = pair[0]\n",
        "         V[(t1,t,i)] = pair[1] \n",
        "         this_viterbi[t] = pair[1]\n",
        "\n",
        "     currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
        "     cur_vit.append(this_viterbi)\n",
        "\n",
        "    backpointer.append(Back_ptr.values())\n",
        "    printV(sentence,tag_list,V,Back_ptr)\n",
        "    print(backpointer)\n",
        "    return backpointer.reverse()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XJ7Cj0QYCXe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "be3e763f-ecde-4274-f078-0e4405b4eda9"
      },
      "source": [
        "tags = viterbi(X_train[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['N', dict_values(['N', 'N', 'N', 'N', 'N', 'S', 'S', 'S', 'S', 'S', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMvh86S-Ze-r"
      },
      "source": [
        "tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsEgotQFeMlM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f7d5798-7b4a-424d-cff5-5a210e92684c"
      },
      "source": [
        "#beginning of anything goes implementation\n",
        "#The model implemented below is taken from the example found online - https://www.depends-on-the-definition.com/guide-sequence-tagging-neural-networks-python/ \n",
        "\n",
        "#calculate unique list of words\n",
        "word_set = list(set([w for sent in dataset for (w,t) in sent]))\n",
        "#word_set.append('ENDPAD')\n",
        "\n",
        "#calculate unique list of tags\n",
        "tag_set = list(set([t for sent in dataset for (w,t) in sent]))\n",
        "#tag_set.append('ENDPAD')\n",
        "#use indices from the word & tag list to create an index dictionary\n",
        "word_idx = {w: i for i, w in enumerate(word_set)} \n",
        "tag_idx = {t: i for i, t in enumerate(tag_set)}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfegCp-QoVCf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49833019-289f-4c76-8ef4-1748f0971a39"
      },
      "source": [
        "tag_idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'H': 4, 'N': 2, 'S': 1, 'T': 3, 'U': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 425
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTPLU-QT2ofx"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxLength =50\n",
        "#keeping the max length of the sentence to 50 \n",
        "#the following lines will create an index vector for text and labels \n",
        "#and add padding to the sentences\n",
        "#padded sentences are assigned 'N' tag\n",
        "X = [[word_idx[w[0]] for w in s] for s in dataset]\n",
        "X = pad_sequences(maxlen=maxLength, sequences=X, padding=\"post\", value=len(word_set) - 1)\n",
        "y = [[tag_idx[w[1]] for w in s] for s in dataset]\n",
        "y = pad_sequences(maxlen=maxLength, sequences=y, padding=\"post\", value=tag_idx[\"N\"])\n",
        "#create one hot vector of labels\n",
        "y = [to_categorical(i, num_classes=len(tag_set)) for i in y]\n",
        "\n",
        "#split data set into train and validation\n",
        "X_train,X_val, y_train, y_val = train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifGLYDq5qV-D"
      },
      "source": [
        "#using Keras to build an RNN\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "max_len =50\n",
        "input = Input(shape=(max_len,))\n",
        "model = Embedding(input_dim=len(word_set), output_dim=50, input_length=max_len)(input)  # 50-dim embedding\n",
        "model = Dropout(0.1)(model)\n",
        "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.2))(model)  # variational biLSTM\n",
        "out = TimeDistributed(Dense(len(tag_set), activation=\"softmax\"))(model)  # softmax output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoKdtTuHq1EJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "14a7d9cd-cabf-4149-e53d-8426b32115c9"
      },
      "source": [
        "#training the model\n",
        "#doesnt seem to complete on colab even with smaller batch size. \n",
        "#was showing promising results on an earlier run but stopped working\n",
        "model = Model(input, out)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, np.array(y_train), batch_size=5, epochs=5, validation_split=0.1, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "19684/57013 [=========>....................] - ETA: 1:51:36 - loss: 0.0753 - accuracy: 0.9742"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFzKm9rYYCXg"
      },
      "source": [
        "# argument sent is a list of [token,label] pairs; return number of correctly predicted labels\n",
        "def predict_from_scratch(sent):\n",
        "  correct = 0\n",
        "  test_sent = ''\n",
        "  for i in range(len(sent)):\n",
        "    \n",
        "    if guess == token[1]:\n",
        "      correct += 1\n",
        "  return correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vez1n5OfYCXj"
      },
      "source": [
        "# argument sent is a list of [token,label] pairs; return number of correctly predicted labels\n",
        "def predict_anything_goes(sent):\n",
        "  correct = 0\n",
        "  for token in sent:\n",
        "    guess = 'N'\n",
        "    if guess == token[1]:\n",
        "      correct += 1\n",
        "  return correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QONu1VYCXl"
      },
      "source": [
        "def evaluate():\n",
        "  total = 0\n",
        "  correct_from_scratch = 0\n",
        "  correct_anything_goes = 0\n",
        "  testfile = open('test.tsv', 'r')\n",
        "  sentence = []\n",
        "  for line in testfile:\n",
        "    total += 1\n",
        "    pieces = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "    if pieces[0]=='<S>':\n",
        "      correct_from_scratch += predict_from_scratch(sentence)\n",
        "      correct_anything_goes += predict_anything_goes(sentence)\n",
        "      sentence = []\n",
        "    else:\n",
        "      sentence.append(pieces)\n",
        "  correct_from_scratch += predict_from_scratch(sentence)\n",
        "  correct_anything_goes += predict_anything_goes(sentence)\n",
        "  return (correct_from_scratch/total, correct_anything_goes/total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn5bkKAOYCXn",
        "outputId": "5a7c8a30-de91-43e4-b9fc-4e1e4d8abdd8"
      },
      "source": [
        "evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.191809, 0.819613)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}