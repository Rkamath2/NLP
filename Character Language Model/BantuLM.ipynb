{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Copy of BantuLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0E1DZpAKAyf"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from math import log2\n",
        "vocabulary = ' !\"\\'(),-.0123456789:;?abcdefghijklmnopqrstuvwxyz'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oia9KX7Jf47P",
        "outputId": "db82fc4c-c8e8-4cda-dec9-0bb4f06eca89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#mount\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe8eINU2gBGd",
        "outputId": "c68c878f-f29a-4233-c41d-8f723dc3ac71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#move to current working directory\n",
        "work_dir = '/content/drive/My Drive/Colab Notebooks/NLP/Bantu Language Modeling/'\n",
        "os.chdir(work_dir)\n",
        "%ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " BantuLM.ipynb            model_cwe.h5   \u001b[0m\u001b[01;34mtest-04\u001b[0m/\n",
            "'Copy of BantuLM.ipynb'   model_sw.h5    \u001b[01;34mtrain-04\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEPzFszOKLv6",
        "outputId": "90c4e01c-98e4-44e9-bf85-89e62fc43f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Removing multiple dots and punctuations from training text. was mostly seen only in the swahili text.\n",
        "def preprocess_data(data):\n",
        "  text_replaced = re.sub(r'\\.+', \".\", data)\n",
        "  text_replaced = re.sub(r'\\!+', \"!\", text_replaced)\n",
        "  return text_replaced\n",
        "\n",
        "\n",
        "def load_file(fileName):\n",
        "  file = open('train-04/'+fileName, 'r')\n",
        "  data = file.read()\n",
        "  file.close()\n",
        "  data = preprocess_data(data)\n",
        "  return data\n",
        "\n",
        "traindata_sw = load_file('sw-train.txt')\n",
        "traindata_cwe = load_file('cwe-train.txt')\n",
        "len(traindata_sw),len(traindata_cwe)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39120830, 603432)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3ntDkanI6J6",
        "outputId": "362d7607-88b2-4aad-ba09-6f112d92fe15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#splitting the training data into sentences based on the end of line punctuation\n",
        "sequences_sw = [list(sent+'.') for sent in traindata_sw.split('. ') if len(sent) > 0]\n",
        "sequences_sw=sequences_sw[:-1]\n",
        "#splitting sentence dataset into training and validation\n",
        "split_index = int(len(sequences_sw) * 0.9)\n",
        "train_sequences_sw = sequences_sw[:split_index]\n",
        "val_sequences_sw = sequences_sw[split_index:]    \n",
        "\n",
        "print(len(train_sequences_sw), len(val_sequences_sw))\n",
        "\n",
        "sequences_cwe = [list(sent+'.') for sent in traindata_cwe.split('. ') if len(sent) > 0]\n",
        "sequences_cwe=sequences_cwe[:-1]\n",
        "#splitting sentence dataset into training and validation\n",
        "split_index = int(len(sequences_cwe) * 0.9)\n",
        "train_sequences_cwe = sequences_cwe[:split_index]\n",
        "val_sequences_cwe = sequences_cwe[split_index:]    \n",
        "\n",
        "print(len(train_sequences_cwe), len(val_sequences_cwe))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "303989 33777\n",
            "4188 466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnjrP9k1hNqW"
      },
      "source": [
        "#beginning of from scratch implementation\n",
        "#Creating the tuple of ngrams in dataset\n",
        "def calc_ngrams(sentence, n,freq_dict):\n",
        "  #print(sentence)\n",
        "  #if n ==1:\n",
        "  #  for i in range(len(sentence)):\n",
        "  #    freq_dict[sentence[i]]+=1\n",
        "  #else:\n",
        "  for i in range(len(sentence)-n+1):\n",
        "    freq_dict[tuple(sentence[i:(i+n)])]+=1\n",
        "  return freq_dict\n",
        "  \n",
        "def create_ngrams(dataset,N):\n",
        "  list_dict = []    #list of dictionaries with ngram counts\n",
        "  #create a dictionary for every N gram and append to list \n",
        "  for i in range(N):\n",
        "    n_freq_dict = defaultdict(int)\n",
        "    for sent in dataset:\n",
        "      temp_sent= sent\n",
        "      #inserting a start of sentence character.\n",
        "      if sent[0]!='<s>':\n",
        "        sent.insert(0,'<s>')\n",
        "      n_freq_dict = calc_ngrams( sent[i:],i+1 ,n_freq_dict)\n",
        "    list_dict.append(n_freq_dict)\n",
        "\n",
        "  return list_dict\n",
        "\n",
        "#calculate Ngram counts for Swahili\n",
        "list_counts_sw = create_ngrams(train_sequences_sw,N_sw)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2ZGieSWCZYC"
      },
      "source": [
        "#calculate Ngram counts for kwere\n",
        "list_counts_cwe = create_ngrams(train_sequences_cwe,N_cwe)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vKO0mT4-zjF"
      },
      "source": [
        "#Calculate probabilities using add alpha Smoothing \n",
        "#alpha was varied for different values. alpha = 0.75 seemed to give the best results. \n",
        "def ngram_prob(list_counts,alpha = 0.75):\n",
        "  V = len(vocabulary)  \n",
        "  list_prob = []\n",
        "  for i in range(len(list_counts)):\n",
        "    prob_dict= defaultdict(int)\n",
        "    #calculate probablities for each N gram character sequences with smoothing\n",
        "    if i==0: \n",
        "      prob_dict = {k: (v+alpha)/(sum(list_counts[i].values())+ alpha*V) for k,v in list_counts[i].items()}\n",
        "    else:\n",
        "      prob_dict = {k: (v+alpha)/(list_counts[i-1][(k[1:])]+ alpha*V) for k,v in list_counts[i].items()}\n",
        "    list_prob.append(prob_dict)\n",
        "  \n",
        "  return list_prob\n",
        "#calculate Ngram probabilties for Swahili\n",
        "list_prob_sw = ngram_prob(list_counts_sw)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NT32ZAGFMrz"
      },
      "source": [
        "#calculate Ngram probabilties for kwere\n",
        "list_prob_cwe =ngram_prob(list_counts_cwe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQLUtJAAGK-A"
      },
      "source": [
        "#Calculations for simple Interpolation for model \n",
        "#This function uses static values of lambda to calculate the probablities for a character based on N -1 history. \n",
        "#the probabilities are then used to calculate loss for each character. \n",
        "#The following link was referred to for the calculations https://github.com/annieyan/language_model/blob/master/yanan_lm.py\n",
        "def interpolatedProbability(test_set,list_prob, lambdas,N):\n",
        "  count = 0 \n",
        "  p=0\n",
        "  loss_from_scratch=0\n",
        "  for sent in test_set:\n",
        "    # insert SOS characters to match training data\n",
        "    if sent[0]!='<s>':\n",
        "      sent.insert(0,'<s>') \n",
        "    for i in range(len(sent)):\n",
        "      p=0 \n",
        "      for j in range(N):\n",
        "        if i >=j:\n",
        "          #Calculating the probability of the character given N history. \n",
        "          #Since the start of sentence symbol is inserted, i+1 is used below to start from the first character.\n",
        "          #print(list_prob[j].get((tuple(sent[i-j:i+1])),0),(tuple(sent[i-j:i+1])))\n",
        "          #P(c)*Lambda1 + P(c|c-1) * lambda2 + P(c|c-1,c-2) * lambda2 ...... \n",
        "          p += (float(lambdas[j]) * list_prob[j].get((tuple(sent[i-j:i+1])),0))\n",
        "      loss_from_scratch -= log2(p)\n",
        "      count+=1\n",
        "  return loss_from_scratch, count\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnHejKlFyXVw",
        "outputId": "3f459e5b-1827-4b6d-dc9f-3bea61e4212b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#define order of Ngram model\n",
        "N_sw =8\n",
        "lambdas1 = [0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2]\n",
        "#calculate Ngram counts for Swahili\n",
        "list_counts_sw = create_ngrams(train_sequences_sw,N_sw)\n",
        "list_prob_sw = ngram_prob(list_counts_sw)\n",
        "\n",
        "log_loss_sw,count_sw = interpolatedProbability(val_sequences_sw,list_prob_sw, lambdas1,N_sw)\n",
        "log_loss_sw/count_sw"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1866683546841745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xypaHrI1J4pe",
        "outputId": "ff1ce59e-c63a-4bed-d1a5-f6d09c151f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#define order of Ngram model\n",
        "N_sw =10\n",
        "#calculate Ngram counts for Swahili\n",
        "list_counts_sw = create_ngrams(train_sequences_sw,N_sw)\n",
        "list_prob_sw = ngram_prob(list_counts_sw)\n",
        "\n",
        "#Calculating entropy for Swahili validation text. \n",
        "#Different distributions of lambda were used. The lambdas1 value gave the best value for entropy in the experimentation\n",
        "lambdas1 = [0.05,0.05,0.05,0.05,0.1,0.1,0.1,0.1,0.2,0.2]\n",
        "lambdas2 = [0.2,0.2,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05]\n",
        "\n",
        "log_loss_sw,count_sw = interpolatedProbability(val_sequences_sw,list_prob_sw, lambdas2,N_sw)\n",
        "log_loss_sw/count_sw"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3611536105375546"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhjrAhBHxmrt",
        "outputId": "eee50382-df24-44b5-fa93-4eeb42c0db4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#define order of Ngram model\n",
        "N_cwe =8\n",
        "#calculate Ngram counts for kwere\n",
        "list_counts_cwe = create_ngrams(train_sequences_cwe,N_cwe)\n",
        "\n",
        "#calculate Ngram probabilties for kwere\n",
        "list_prob_cwe =ngram_prob(list_counts_cwe)\n",
        "lambdas1 = [0.1,0.1,0.1,0.1,0.1,0.1,0.2,0.2]\n",
        "\n",
        "log_loss_cwe,count_cwe = interpolatedProbability(val_sequences_cwe,list_prob_cwe,lambdas1,N_cwe)\n",
        "log_loss_cwe/(count_cwe)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4319636104557056"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTGuHhE1yXJ7",
        "outputId": "30a5ce33-df21-465e-da0f-f357cf4c8cda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#define order of Ngram model\n",
        "N_cwe =10\n",
        "#calculate Ngram counts for kwere\n",
        "list_counts_cwe = create_ngrams(train_sequences_cwe,N_cwe)\n",
        "\n",
        "#calculate Ngram probabilties for kwere\n",
        "list_prob_cwe =ngram_prob(list_counts_cwe)\n",
        "#Calculating entropy for Swahili validation text. \n",
        "lambdas1 = [0.05,0.05,0.05,0.05,0.1,0.1,0.1,0.1,0.2,0.2]\n",
        "lambdas2 = [0.2,0.2,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05]\n",
        "\n",
        "log_loss_cwe,count_cwe = interpolatedProbability(val_sequences_cwe,list_prob_cwe,lambdas2,N_cwe)\n",
        "log_loss_cwe/(count_cwe)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.481591813484035"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93gA16ZmlL0Y",
        "outputId": "59841b2d-00fe-49a3-b003-aec9e0c89e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Beginning of Anything goes\n",
        "#redoing this since I dont want the start of sentence symbols for the CNN \n",
        "sequences_model_sw = [list(sent+'.') for sent in traindata_sw.split('. ') if len(sent) > 0]\n",
        "sequences_model_sw=sequences_model_sw[:-1]      #remove the last line since its a newline character\n",
        "#splitting sentence dataset \n",
        "split_index = int(len(sequences_model_sw) * 0.9)\n",
        "train_model_sw = sequences_model_sw[:split_index]\n",
        "val_model_sw = sequences_model_sw[split_index:]    \n",
        "\n",
        "print(len(train_model_sw), len(val_model_sw))\n",
        "\n",
        "sequences_model_cwe = [list(sent+'.') for sent in traindata_cwe.split('. ') if len(sent) > 0]\n",
        "sequences_model_cwe=sequences_model_cwe[:-1]\n",
        "#splitting sentence dataset \n",
        "split_index = int(len(sequences_model_cwe) * 0.9)\n",
        "train_model_cwe = sequences_model_cwe[:split_index]\n",
        "val_model_cwe = sequences_model_cwe[split_index:]    \n",
        "\n",
        "print(len(train_model_cwe), len(val_model_cwe))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "303989 33777\n",
            "4188 466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh41XtWJSG28"
      },
      "source": [
        "#Since we have fixed vocabulary of characters.\n",
        "vocabulary = list(vocabulary)\n",
        "\n",
        "vocabulary.append('<PAD>')\n",
        "#convert the vocabulary to indexes\n",
        "char_idx = {c: i for i, c in enumerate(vocabulary)} "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLHyYuELlY-3"
      },
      "source": [
        "#batch generator code for the text\n",
        "#This was done since the training set generated was too high to send to the CNN model directly. \n",
        "#Some of the code for generator is adapated from Dr.Scannells code and the logic for creating sequences is taken from - https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/\n",
        "\n",
        "def batch_generator(data,batch_size,max_length): \n",
        "  #use the sequences and map them to indices to create the training set\n",
        "  curr_seq2idx = [[char_idx[c[0]] for c in s] for s in data] \n",
        "  i = 0\n",
        "  while True:\n",
        "    X=[]\n",
        "    y=[]\n",
        "    sequences = list()\n",
        "    labels = list()\n",
        "    while len(labels) < batch_size:\n",
        "      if i == len(data):\n",
        "          i=0           \n",
        "      # create line-based sequences\n",
        "      line = curr_seq2idx[i]\n",
        "      line.insert(0,char_idx['<PAD>'])\n",
        "      \n",
        "      #Creating a sliding window sequence set for each sentence.\n",
        "      #where the next character is the prediction label \n",
        "      for j in range(0, len(line)):\n",
        "        sequence = line[:j+1]\n",
        "        #print(i)\n",
        "        if j < len(line)-1:\n",
        "          label=line[j+1]\n",
        "        else:\n",
        "          label=char_idx['<PAD>']       #end of sentence will have padded character as label\n",
        "        print(sequence,label)\n",
        "        sequences.append(sequence)\n",
        "        labels.append(label)\n",
        "      #print('Total Sequences: %d' % len(sequences),len(labels))\n",
        "      i+=1\n",
        "     # pre padding input sequences with <PAD> character\n",
        "    sequences = pad_sequences(sequences, maxlen=max_length, padding='pre',value=char_idx['<PAD>'])\n",
        "    # split into input and output elements\n",
        "    X = np.array(sequences)\n",
        "    y = to_categorical(labels, num_classes=len(vocabulary))\n",
        "    #print(X.shape,y.shape)\n",
        "    yield (X,y)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNcTciE00h9I"
      },
      "source": [
        "#Building the Training model using Keras\n",
        "#This model has been taken from an online code but I cant find the reference link to it anymore.\n",
        "from tensorflow.keras.layers import Dense, Embedding, Activation, Flatten,LSTM,Convolution1D,MaxPooling1D\n",
        "from tensorflow.keras import Sequential,Model\n",
        "max_len =100\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=len(vocabulary), output_dim=100, input_length=max_len))\n",
        "  model.add(Convolution1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(len(vocabulary), activation='softmax'))\n",
        "  model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "  model.summary()\n",
        "  return model "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DqKU9Ak0G1B",
        "outputId": "f2966e9d-9130-4986-e9e3-4727c18e8bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#the is the fit_generator method which splits the data in batches using the batchsize for quicker training purposes \n",
        "#Model training for Swahili text\n",
        "model_sw = create_model()\n",
        "batch_size = 1024\n",
        "totalnum =len(train_model_sw)\n",
        "print(len(char_idx))\n",
        "#model.fit_generator(example_generator_file(train_model_sw[:1000],batch_size, totalnum, char_idx, max_len), steps_per_epoch=totalnum//batch_size, epochs=2, verbose=1)\n",
        "model_sw.fit_generator(batch_generator(train_model_sw,batch_size,max_len),steps_per_epoch=totalnum//batch_size, epochs=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 100, 100)          4900      \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 100, 32)           9632      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1 (None, 50, 32)            0         \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 49)                4949      \n",
            "=================================================================\n",
            "Total params: 72,681\n",
            "Trainable params: 72,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "49\n",
            "Epoch 1/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 2.6508\n",
            "Epoch 2/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 2.2163\n",
            "Epoch 3/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 2.1306\n",
            "Epoch 4/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 2.0847\n",
            "Epoch 5/100\n",
            "296/296 [==============================] - 12s 40ms/step - loss: 2.0364\n",
            "Epoch 6/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 2.0093\n",
            "Epoch 7/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.9860\n",
            "Epoch 8/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.9649\n",
            "Epoch 9/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.9433\n",
            "Epoch 10/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.9278\n",
            "Epoch 11/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.9108\n",
            "Epoch 12/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.8952\n",
            "Epoch 13/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.8796\n",
            "Epoch 14/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.8750\n",
            "Epoch 15/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.8629\n",
            "Epoch 16/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.8477\n",
            "Epoch 17/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.8403\n",
            "Epoch 18/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.8278\n",
            "Epoch 19/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.8273\n",
            "Epoch 20/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.8161\n",
            "Epoch 21/100\n",
            "296/296 [==============================] - 10s 32ms/step - loss: 1.8085\n",
            "Epoch 22/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.8030\n",
            "Epoch 23/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7955\n",
            "Epoch 24/100\n",
            "296/296 [==============================] - 9s 32ms/step - loss: 1.7888\n",
            "Epoch 25/100\n",
            "296/296 [==============================] - 10s 32ms/step - loss: 1.7899\n",
            "Epoch 26/100\n",
            "296/296 [==============================] - 9s 32ms/step - loss: 1.7805\n",
            "Epoch 27/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7759\n",
            "Epoch 28/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7717\n",
            "Epoch 29/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.7650\n",
            "Epoch 30/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7701\n",
            "Epoch 31/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7572\n",
            "Epoch 32/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7534\n",
            "Epoch 33/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7431\n",
            "Epoch 34/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7445\n",
            "Epoch 35/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7456\n",
            "Epoch 36/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7358\n",
            "Epoch 37/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7409\n",
            "Epoch 38/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7298\n",
            "Epoch 39/100\n",
            "296/296 [==============================] - 12s 40ms/step - loss: 1.7252\n",
            "Epoch 40/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7239\n",
            "Epoch 41/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7237\n",
            "Epoch 42/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7144\n",
            "Epoch 43/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7133\n",
            "Epoch 44/100\n",
            "296/296 [==============================] - 9s 32ms/step - loss: 1.7166\n",
            "Epoch 45/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7140\n",
            "Epoch 46/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7184\n",
            "Epoch 47/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7107\n",
            "Epoch 48/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7015\n",
            "Epoch 49/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7016\n",
            "Epoch 50/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7021\n",
            "Epoch 51/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7043\n",
            "Epoch 52/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.7013\n",
            "Epoch 53/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6945\n",
            "Epoch 54/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6962\n",
            "Epoch 55/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6947\n",
            "Epoch 56/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6998\n",
            "Epoch 57/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6916\n",
            "Epoch 58/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6879\n",
            "Epoch 59/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6912\n",
            "Epoch 60/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6861\n",
            "Epoch 61/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6870\n",
            "Epoch 62/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6832\n",
            "Epoch 63/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6823\n",
            "Epoch 64/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6753\n",
            "Epoch 65/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6923\n",
            "Epoch 66/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6895\n",
            "Epoch 67/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6793\n",
            "Epoch 68/100\n",
            "296/296 [==============================] - 9s 32ms/step - loss: 1.6848\n",
            "Epoch 69/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6766\n",
            "Epoch 70/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6632\n",
            "Epoch 71/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6782\n",
            "Epoch 72/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6861\n",
            "Epoch 73/100\n",
            "296/296 [==============================] - 12s 40ms/step - loss: 1.6700\n",
            "Epoch 74/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6672\n",
            "Epoch 75/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6744\n",
            "Epoch 76/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6701\n",
            "Epoch 77/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6611\n",
            "Epoch 78/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6736\n",
            "Epoch 79/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6672\n",
            "Epoch 80/100\n",
            "296/296 [==============================] - 9s 32ms/step - loss: 1.6663\n",
            "Epoch 81/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6678\n",
            "Epoch 82/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6731\n",
            "Epoch 83/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6566\n",
            "Epoch 84/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6652\n",
            "Epoch 85/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6613\n",
            "Epoch 86/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6641\n",
            "Epoch 87/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6548\n",
            "Epoch 88/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6475\n",
            "Epoch 89/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6595\n",
            "Epoch 90/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6546\n",
            "Epoch 91/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6574\n",
            "Epoch 92/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6591\n",
            "Epoch 93/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6612\n",
            "Epoch 94/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6502\n",
            "Epoch 95/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6499\n",
            "Epoch 96/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6478\n",
            "Epoch 97/100\n",
            "296/296 [==============================] - 9s 30ms/step - loss: 1.6596\n",
            "Epoch 98/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6532\n",
            "Epoch 99/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6450\n",
            "Epoch 100/100\n",
            "296/296 [==============================] - 9s 31ms/step - loss: 1.6541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f286d277ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7-4R1va2yNN",
        "outputId": "a3e94155-99b2-41b3-8337-73d5e900f3e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# save the model to file\n",
        "model_sw.save('model_sw.h5')\n",
        "\n",
        "#evaluating the loss on validation set for Swahili\n",
        "model_sw.evaluate_generator(batch_generator(val_model_sw,batch_size,max_len), 1000,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 16s 16ms/step - loss: 1.6501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.650132179260254"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvGxNYOf3Iyl",
        "outputId": "c5f5a48d-6905-4a79-a233-717078072b20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Prediction on validation set\n",
        "#Sends the data in batches and the predictions are retrieved from each batch\n",
        "#the prediction method below is adapted from - https://github.com/jsilter/dbpedia_classify/blob/part1/keras_text_classification.py\n",
        "\n",
        "batch_size =1024\n",
        "\n",
        "pred_generator_sw = batch_generator(val_model_sw,batch_size,max_len)\n",
        "max_to_pred_sw = len(val_model_sw)\n",
        "num_predded_sw = 0\n",
        "pred_res_sw = []\n",
        "for pred_inputs in pred_generator_sw:\n",
        "    X_val, y_val = pred_inputs\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    #offset = num_predded\n",
        "    num_predded_sw += len(y_pred)\n",
        "\n",
        "    for y in y_pred:\n",
        "      pred_res_sw.append(y)\n",
        "    #print(pred_res)\n",
        "    #if data already predicted is greater than length of validation set, then break loop\n",
        "    #this is done so that the prediction calculations are not repeated on the data\n",
        "    if (num_predded_sw + batch_size) > max_to_pred_sw:\n",
        "      break\n",
        "\n",
        "loss_from_val = 0\n",
        "for i in range (len(pred_res_sw)-1):\n",
        "  p = max(pred_res_sw[i])\n",
        "  #print(i)\n",
        "  loss_from_val -= log2(p)\n",
        "\n",
        "#Cross entropy on Swahili validation set\n",
        "loss_from_val/len(pred_res_sw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.3240622709055145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JiCylZjdqNe",
        "outputId": "a12b2132-3b9e-4384-8f2d-853a1019fb17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Model training for Swahili text\n",
        "model_cwe= create_model()\n",
        "batch_size = 1024\n",
        "totalnum =len(train_model_cwe)\n",
        "print(len(char_idx))\n",
        "#model.fit_generator(example_generator_file(train_model_sw[:1000],batch_size, totalnum, char_idx, max_len), steps_per_epoch=totalnum//batch_size, epochs=2, verbose=1)\n",
        "model_cwe.fit_generator(batch_generator(train_model_cwe,batch_size,max_len),steps_per_epoch=totalnum//batch_size, epochs=200, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 100, 100)          4900      \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 100, 32)           9632      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling (None, 50, 32)            0         \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 49)                4949      \n",
            "=================================================================\n",
            "Total params: 72,681\n",
            "Trainable params: 72,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "49\n",
            "Epoch 1/200\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 3.8852\n",
            "Epoch 2/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 3.8407\n",
            "Epoch 3/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 3.6772\n",
            "Epoch 4/200\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 3.2793\n",
            "Epoch 5/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 3.0970\n",
            "Epoch 6/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 3.0313\n",
            "Epoch 7/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.9829\n",
            "Epoch 8/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.9759\n",
            "Epoch 9/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.9558\n",
            "Epoch 10/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.9241\n",
            "Epoch 11/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.9391\n",
            "Epoch 12/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.9212\n",
            "Epoch 13/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.9276\n",
            "Epoch 14/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.9105\n",
            "Epoch 15/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.8942\n",
            "Epoch 16/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.9126\n",
            "Epoch 17/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.8847\n",
            "Epoch 18/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 2.9065\n",
            "Epoch 19/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.8741\n",
            "Epoch 20/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.8864\n",
            "Epoch 21/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.8486\n",
            "Epoch 22/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.8309\n",
            "Epoch 23/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.8114\n",
            "Epoch 24/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.7557\n",
            "Epoch 25/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.7043\n",
            "Epoch 26/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.6716\n",
            "Epoch 27/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.6165\n",
            "Epoch 28/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.5636\n",
            "Epoch 29/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.5254\n",
            "Epoch 30/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.4810\n",
            "Epoch 31/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.4171\n",
            "Epoch 32/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.3524\n",
            "Epoch 33/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.3638\n",
            "Epoch 34/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.3254\n",
            "Epoch 35/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.2903\n",
            "Epoch 36/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.2677\n",
            "Epoch 37/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.2496\n",
            "Epoch 38/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.2191\n",
            "Epoch 39/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 2.2224\n",
            "Epoch 40/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 2.1809\n",
            "Epoch 41/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.1745\n",
            "Epoch 42/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.1887\n",
            "Epoch 43/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 2.1845\n",
            "Epoch 44/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.1239\n",
            "Epoch 45/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.1232\n",
            "Epoch 46/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.1388\n",
            "Epoch 47/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.1206\n",
            "Epoch 48/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0984\n",
            "Epoch 49/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.1189\n",
            "Epoch 50/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.0697\n",
            "Epoch 51/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.0788\n",
            "Epoch 52/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0654\n",
            "Epoch 53/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0587\n",
            "Epoch 54/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0656\n",
            "Epoch 55/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.0297\n",
            "Epoch 56/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0161\n",
            "Epoch 57/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.0686\n",
            "Epoch 58/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.0306\n",
            "Epoch 59/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 2.0309\n",
            "Epoch 60/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0324\n",
            "Epoch 61/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.0149\n",
            "Epoch 62/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0011\n",
            "Epoch 63/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0175\n",
            "Epoch 64/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0133\n",
            "Epoch 65/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 2.0075\n",
            "Epoch 66/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 2.0010\n",
            "Epoch 67/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9540\n",
            "Epoch 68/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.9863\n",
            "Epoch 69/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.9746\n",
            "Epoch 70/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9640\n",
            "Epoch 71/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.9486\n",
            "Epoch 72/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9547\n",
            "Epoch 73/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.9469\n",
            "Epoch 74/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9548\n",
            "Epoch 75/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.9390\n",
            "Epoch 76/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.9187\n",
            "Epoch 77/200\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 1.9476\n",
            "Epoch 78/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9409\n",
            "Epoch 79/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9255\n",
            "Epoch 80/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.9465\n",
            "Epoch 81/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.9184\n",
            "Epoch 82/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9610\n",
            "Epoch 83/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.9291\n",
            "Epoch 84/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.9073\n",
            "Epoch 85/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.9223\n",
            "Epoch 86/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.9221\n",
            "Epoch 87/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9090\n",
            "Epoch 88/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9406\n",
            "Epoch 89/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9418\n",
            "Epoch 90/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9110\n",
            "Epoch 91/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8842\n",
            "Epoch 92/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.9269\n",
            "Epoch 93/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8756\n",
            "Epoch 94/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9133\n",
            "Epoch 95/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8980\n",
            "Epoch 96/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8890\n",
            "Epoch 97/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8794\n",
            "Epoch 98/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8599\n",
            "Epoch 99/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8387\n",
            "Epoch 100/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8724\n",
            "Epoch 101/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8838\n",
            "Epoch 102/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.8905\n",
            "Epoch 103/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.8894\n",
            "Epoch 104/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9075\n",
            "Epoch 105/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8876\n",
            "Epoch 106/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8822\n",
            "Epoch 107/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8810\n",
            "Epoch 108/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8861\n",
            "Epoch 109/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8349\n",
            "Epoch 110/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8721\n",
            "Epoch 111/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8893\n",
            "Epoch 112/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8499\n",
            "Epoch 113/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8320\n",
            "Epoch 114/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8541\n",
            "Epoch 115/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.8653\n",
            "Epoch 116/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8581\n",
            "Epoch 117/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8665\n",
            "Epoch 118/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8456\n",
            "Epoch 119/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8295\n",
            "Epoch 120/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8109\n",
            "Epoch 121/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8457\n",
            "Epoch 122/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8291\n",
            "Epoch 123/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8965\n",
            "Epoch 124/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.9050\n",
            "Epoch 125/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.8834\n",
            "Epoch 126/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8564\n",
            "Epoch 127/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8629\n",
            "Epoch 128/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8582\n",
            "Epoch 129/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8226\n",
            "Epoch 130/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8581\n",
            "Epoch 131/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8656\n",
            "Epoch 132/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8122\n",
            "Epoch 133/200\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 1.8378\n",
            "Epoch 134/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.8244\n",
            "Epoch 135/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8045\n",
            "Epoch 136/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.8195\n",
            "Epoch 137/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.8425\n",
            "Epoch 138/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7820\n",
            "Epoch 139/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.7840\n",
            "Epoch 140/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8193\n",
            "Epoch 141/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8058\n",
            "Epoch 142/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8311\n",
            "Epoch 143/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.8256\n",
            "Epoch 144/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8104\n",
            "Epoch 145/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8230\n",
            "Epoch 146/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8102\n",
            "Epoch 147/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8111\n",
            "Epoch 148/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8317\n",
            "Epoch 149/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.7858\n",
            "Epoch 150/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8303\n",
            "Epoch 151/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.8082\n",
            "Epoch 152/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.8038\n",
            "Epoch 153/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.8045\n",
            "Epoch 154/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.7632\n",
            "Epoch 155/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7776\n",
            "Epoch 156/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.8034\n",
            "Epoch 157/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.7785\n",
            "Epoch 158/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7409\n",
            "Epoch 159/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.7607\n",
            "Epoch 160/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7886\n",
            "Epoch 161/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.7598\n",
            "Epoch 162/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7709\n",
            "Epoch 163/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.7673\n",
            "Epoch 164/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7500\n",
            "Epoch 165/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.7749\n",
            "Epoch 166/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7898\n",
            "Epoch 167/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.7161\n",
            "Epoch 168/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.7553\n",
            "Epoch 169/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.8000\n",
            "Epoch 170/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7735\n",
            "Epoch 171/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.7473\n",
            "Epoch 172/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.7745\n",
            "Epoch 173/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.7575\n",
            "Epoch 174/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7552\n",
            "Epoch 175/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7288\n",
            "Epoch 176/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7543\n",
            "Epoch 177/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7474\n",
            "Epoch 178/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7402\n",
            "Epoch 179/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.7063\n",
            "Epoch 180/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7869\n",
            "Epoch 181/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7518\n",
            "Epoch 182/200\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 1.7487\n",
            "Epoch 183/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7334\n",
            "Epoch 184/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7374\n",
            "Epoch 185/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.7348\n",
            "Epoch 186/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.7705\n",
            "Epoch 187/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.7356\n",
            "Epoch 188/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.7578\n",
            "Epoch 189/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7397\n",
            "Epoch 190/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7114\n",
            "Epoch 191/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.7378\n",
            "Epoch 192/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.7255\n",
            "Epoch 193/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7007\n",
            "Epoch 194/200\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.7035\n",
            "Epoch 195/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7148\n",
            "Epoch 196/200\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 1.7221\n",
            "Epoch 197/200\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 1.7088\n",
            "Epoch 198/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.6993\n",
            "Epoch 199/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.6908\n",
            "Epoch 200/200\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 1.7207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f26a5fe2b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCoeRe-wPsra",
        "outputId": "e0668768-8206-4599-ffef-d12e9be49376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_cwe.save('model_cwe.h5')\n",
        "model_cwe.evaluate_generator(batch_generator(val_model_cwe,batch_size,max_len), len(val_model_cwe),verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "466/466 [==============================] - 8s 17ms/step - loss: 1.6886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.688645362854004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJPz_UosTzXw",
        "outputId": "6bda0aba-c72c-4098-96d2-2de970d966dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Prediction on validation set\n",
        "#Its the same as Swahili validation set prediction \n",
        "batch_size =1024\n",
        "\n",
        "pred_generator_cwe = batch_generator(val_model_sw,batch_size,max_len)\n",
        "max_to_pred = len(val_model_cwe)\n",
        "num_predded_cwe = 0\n",
        "pred_res_cwe = []\n",
        "for pred_inputs in pred_generator_cwe:\n",
        "    X_val, y_val = pred_inputs\n",
        "    y_pred = model_cwe.predict(X_val)\n",
        "\n",
        "    offset = num_predded_cwe\n",
        "    num_predded_cwe += len(y_pred)\n",
        "\n",
        "    for y in y_pred:\n",
        "      pred_res_cwe.append(y)\n",
        "    #print(pred_res)\n",
        "    if (num_predded_cwe + batch_size) > max_to_pred:\n",
        "      break\n",
        "      \n",
        "loss_val_cwe = 0\n",
        "for i in range (len(pred_res_cwe)-1):\n",
        "  p = max(pred_res_cwe[i])\n",
        "  #print(i)\n",
        "  loss_val_cwe -= log2(p)\n",
        "\n",
        "#Cross entropy on kwere validation set\n",
        "loss_val_cwe/len(pred_res_cwe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4753879568720603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBUJFnNWKAyi"
      },
      "source": [
        "# returns a probability in (0,1)\n",
        "# return values must sum to 1.0 over all possible characters c\n",
        "def from_scratch_model(lang, test_sequences):\n",
        "  #same as validation lambda\n",
        "  lambdas1 = [0.5,0.5,0.5,0.5,0.1,0.1,0.1,0.1,0.2,0.2]\n",
        "  if lang == 'sw':\n",
        "    log_loss,count = interpolatedProbability(test_sequences,list_prob_sw, lambdas1,N_sw)\n",
        "  else:\n",
        "    log_loss,count = interpolatedProbability(test_sequences,list_prob_cwe, lambdas1,N_cwe)\n",
        "\n",
        "  return log_loss,count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTl8RDQKKAyl"
      },
      "source": [
        "# returns a probability in (0,1)\n",
        "# return values must sum to 1.0 over all possible characters c\n",
        "def anything_goes_model(lang, test_sequences):\n",
        "  #Prediction on validation set\n",
        "  batch_size =1024\n",
        "  max_len =100\n",
        "  pred_generator = batch_generator(test_sequences,batch_size,max_len)\n",
        "  max_to_pred = len(test_sequences)\n",
        "  num_predded = 0\n",
        "  pred_res = []\n",
        "  for pred_inputs in pred_generator:\n",
        "      X_val, y_val = pred_inputs\n",
        "      if lang == 'cwe':\n",
        "        y_pred = model_cwe.predict(X_val)\n",
        "      else:\n",
        "        y_pred = model_sw.predict(X_val)\n",
        "\n",
        "      #offset = num_predded_cwe\n",
        "      num_predded += len(y_pred)\n",
        "\n",
        "      for y in y_pred:\n",
        "        pred_res.append(y)\n",
        "      #print(pred_res)\n",
        "      if (num_predded + batch_size) > max_to_pred:\n",
        "        break\n",
        "          \n",
        "  loss_test = 0\n",
        "  for i in range (len(pred_res)-1):\n",
        "    p = max(pred_res[i])\n",
        "    #print(i)\n",
        "    loss_test -= log2(p)\n",
        "  return loss_test,len(pred_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKih_4GxKAyn"
      },
      "source": [
        "def evaluate_one(lang):\n",
        "  testfile = open('test-04/'+lang+'-test.txt', 'r')\n",
        "  max_history = 100\n",
        "  history = []\n",
        "  loss_anything_goes = 0\n",
        "  loss_from_scratch = 0\n",
        "  count_a =0 \n",
        "  count_s = 0\n",
        "  test_data = testfile.read()\n",
        "  #same preprocess as training\n",
        "  test_data = preprocess_data(test_data)\n",
        "\n",
        "  #splitting into sentences similar to training data\n",
        "  test_sequences = [list(sent+'.') for sent in test_data.split('. ') if len(sent) > 0]\n",
        "  test_sequences=test_sequences[:-1]\n",
        "  loss_anything_goes,count_a = anything_goes_model(lang,test_sequences)\n",
        "  loss_from_scratch,count_s = from_scratch_model(lang,test_sequences)\n",
        "  \n",
        "  return [loss_from_scratch/count_s, loss_anything_goes/count_a]\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-i81qXlKAyp"
      },
      "source": [
        "def evaluate():\n",
        "  ans = evaluate_one('cwe')\n",
        "  ans.extend(evaluate_one('sw'))\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRsM42cPKAys",
        "outputId": "db971bb5-5ee5-42fc-d11a-f417fe20a4c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0264376538288342, 1.377534831885082, 0.9566084799931058, 1.1896897431879887]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    }
  ]
}